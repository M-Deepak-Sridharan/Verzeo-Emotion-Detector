{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmWLOj3hIVVE"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNqu1eWzIVVH"
      },
      "outputs": [],
      "source": [
        "images=[]\n",
        "targets=[]\n",
        "\n",
        "# {angry : 0, happy : 1, sad : 2,neutral : 3, surprise : 4 } - 5 Diffetent EMOTIONS\n",
        "angry= r'E:\\Major\\images\\train\\angry'\n",
        "happy=r'E:\\Major\\images\\train\\happy'\n",
        "sad=r'E:\\Major\\images\\train\\sad'\n",
        "neutral=r'E:\\Major\\images\\train\\neutral'\n",
        "surprise=r'E:\\Major\\images\\train\\surprise'\n",
        "fear=r'E:\\Major\\images\\train\\fear'\n",
        "disgust=r'E:\\Major\\images\\train\\disgust'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ryFbgZVIVVH"
      },
      "outputs": [],
      "source": [
        "content=os.listdir(angry)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=angry + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(0)\n",
        "    except Exception as e:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faw4ehFjIVVI"
      },
      "outputs": [],
      "source": [
        "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
        "content=os.listdir(happy)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=happy + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(1)\n",
        "    except Exception as e:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHbAeMxAIVVI"
      },
      "outputs": [],
      "source": [
        "content=os.listdir(sad)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=sad + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(2)\n",
        "    except Exception as e:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFodaBJfIVVI"
      },
      "outputs": [],
      "source": [
        "content=os.listdir(neutral)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=neutral + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(3)\n",
        "    except Exception as e:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax7lQTNvIVVI"
      },
      "outputs": [],
      "source": [
        "content=os.listdir(surprise)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=surprise + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(4)\n",
        "    except Exception as e:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YD2IW1VIVVJ"
      },
      "outputs": [],
      "source": [
        "content=os.listdir(fear)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=fear + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(5)\n",
        "    except Exception as e:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QKeJNZtIVVK"
      },
      "outputs": [],
      "source": [
        "content=os.listdir(disgust)\n",
        "\n",
        "for image in content:\n",
        "    try:\n",
        "        image_path=disgust + '\\\\'+ image\n",
        "        image=cv2.imread(image_path)\n",
        "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        resized_image=cv2.resize(image_grey,(48,48))\n",
        "        images.append(resized_image)\n",
        "        targets.append(6)\n",
        "    except Exception as se:\n",
        "        print(\"exception\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lJtxmL4IVVK"
      },
      "outputs": [],
      "source": [
        "images = np.array(images)/255.0\n",
        "targets = np.array(targets)/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y8h8M3-IVVK"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(images, targets, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhPdAPy9IVVK",
        "outputId": "d50482cb-f7c3-4f37-ab9b-283762cc14d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyJ5WB8iIVVL"
      },
      "outputs": [],
      "source": [
        "X_train=X_train.reshape(X_train.shape[0],48, 48, 1)\n",
        "X_test=X_test.reshape(X_test.shape[0],48,48,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L67tgNbIVVL"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "Y_train = label_encoder.fit_transform(Y_train)\n",
        "Y_test=label_encoder.fit_transform(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qdKZk0oIVVL"
      },
      "outputs": [],
      "source": [
        "Y_train=np_utils.to_categorical(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S91a7mHvIVVL"
      },
      "outputs": [],
      "source": [
        "model= Sequential()\n",
        "\n",
        "model.add(Conv2D(200, (3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(150, (3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(100, (3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout((0.4)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "# Compiling Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7r3cmHuIVVL",
        "outputId": "23ceb76c-f129-4fcc-cf2b-b563a8d83657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.7684 - accuracy: 0.2669WARNING:tensorflow:From C:\\Users\\devan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From C:\\Users\\devan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 128s 245ms/step - loss: 1.7684 - accuracy: 0.2669 - val_loss: 1.6709 - val_accuracy: 0.3249\n",
            "Epoch 2/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.5876 - accuracy: 0.3808INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 127s 242ms/step - loss: 1.5876 - accuracy: 0.3808 - val_loss: 1.5077 - val_accuracy: 0.4247\n",
            "Epoch 3/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.4722 - accuracy: 0.4348INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 131s 251ms/step - loss: 1.4722 - accuracy: 0.4348 - val_loss: 1.4053 - val_accuracy: 0.4655\n",
            "Epoch 4/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.3882 - accuracy: 0.4688INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 128s 245ms/step - loss: 1.3883 - accuracy: 0.4688 - val_loss: 1.3457 - val_accuracy: 0.4940\n",
            "Epoch 5/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.3245 - accuracy: 0.4917INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 127s 244ms/step - loss: 1.3245 - accuracy: 0.4917 - val_loss: 1.3341 - val_accuracy: 0.4995\n",
            "Epoch 6/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.2804 - accuracy: 0.5087INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 127s 243ms/step - loss: 1.2802 - accuracy: 0.5088 - val_loss: 1.2795 - val_accuracy: 0.5149\n",
            "Epoch 7/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.2373 - accuracy: 0.5231INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 127s 244ms/step - loss: 1.2373 - accuracy: 0.5231 - val_loss: 1.2459 - val_accuracy: 0.5209\n",
            "Epoch 8/35\n",
            "522/522 [==============================] - ETA: 0s - loss: 1.2008 - accuracy: 0.5444INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 326s 624ms/step - loss: 1.2008 - accuracy: 0.5444 - val_loss: 1.2165 - val_accuracy: 0.5388\n",
            "Epoch 9/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.1609 - accuracy: 0.5567INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 355s 681ms/step - loss: 1.1610 - accuracy: 0.5567 - val_loss: 1.2089 - val_accuracy: 0.5357\n",
            "Epoch 10/35\n",
            "522/522 [==============================] - 135s 258ms/step - loss: 1.1231 - accuracy: 0.5738 - val_loss: 1.2266 - val_accuracy: 0.5348\n",
            "Epoch 11/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.1038 - accuracy: 0.5840INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 127s 243ms/step - loss: 1.1038 - accuracy: 0.5840 - val_loss: 1.1973 - val_accuracy: 0.5540\n",
            "Epoch 12/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.0599 - accuracy: 0.5968INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 128s 246ms/step - loss: 1.0600 - accuracy: 0.5968 - val_loss: 1.1766 - val_accuracy: 0.5597\n",
            "Epoch 13/35\n",
            "521/522 [============================>.] - ETA: 0s - loss: 1.0369 - accuracy: 0.6102INFO:tensorflow:Assets written to: model-ED\\assets\n",
            "522/522 [==============================] - 127s 244ms/step - loss: 1.0368 - accuracy: 0.6102 - val_loss: 1.1687 - val_accuracy: 0.5621\n",
            "Epoch 14/35\n",
            "522/522 [==============================] - 125s 240ms/step - loss: 1.0018 - accuracy: 0.6204 - val_loss: 1.1838 - val_accuracy: 0.5549\n",
            "Epoch 15/35\n",
            "522/522 [==============================] - 125s 240ms/step - loss: 0.9740 - accuracy: 0.6345 - val_loss: 1.1898 - val_accuracy: 0.5588\n",
            "Epoch 16/35\n",
            "522/522 [==============================] - 126s 242ms/step - loss: 0.9535 - accuracy: 0.6403 - val_loss: 1.1948 - val_accuracy: 0.5508\n",
            "Epoch 17/35\n",
            "522/522 [==============================] - 126s 241ms/step - loss: 0.9180 - accuracy: 0.6539 - val_loss: 1.1949 - val_accuracy: 0.5477\n",
            "Epoch 18/35\n",
            "522/522 [==============================] - 124s 238ms/step - loss: 0.8990 - accuracy: 0.6618 - val_loss: 1.2050 - val_accuracy: 0.5544\n",
            "Epoch 19/35\n",
            "522/522 [==============================] - 124s 238ms/step - loss: 0.8679 - accuracy: 0.6728 - val_loss: 1.2014 - val_accuracy: 0.5518\n",
            "Epoch 20/35\n",
            "522/522 [==============================] - 124s 238ms/step - loss: 0.8502 - accuracy: 0.6820 - val_loss: 1.2237 - val_accuracy: 0.5549\n",
            "Epoch 21/35\n",
            "522/522 [==============================] - 125s 239ms/step - loss: 0.8295 - accuracy: 0.6902 - val_loss: 1.2487 - val_accuracy: 0.5463\n",
            "Epoch 22/35\n",
            "522/522 [==============================] - 125s 239ms/step - loss: 0.8078 - accuracy: 0.6954 - val_loss: 1.2425 - val_accuracy: 0.5549\n",
            "Epoch 23/35\n",
            "522/522 [==============================] - 126s 241ms/step - loss: 0.7946 - accuracy: 0.7041 - val_loss: 1.2655 - val_accuracy: 0.5513\n",
            "Epoch 24/35\n",
            "522/522 [==============================] - 125s 239ms/step - loss: 0.7772 - accuracy: 0.7119 - val_loss: 1.2622 - val_accuracy: 0.5475\n",
            "Epoch 25/35\n",
            "522/522 [==============================] - 125s 239ms/step - loss: 0.7480 - accuracy: 0.7201 - val_loss: 1.2607 - val_accuracy: 0.5583\n",
            "Epoch 26/35\n",
            "522/522 [==============================] - 124s 237ms/step - loss: 0.7401 - accuracy: 0.7247 - val_loss: 1.3042 - val_accuracy: 0.5499\n",
            "Epoch 27/35\n",
            "522/522 [==============================] - 124s 238ms/step - loss: 0.7135 - accuracy: 0.7336 - val_loss: 1.2905 - val_accuracy: 0.5602\n",
            "Epoch 28/35\n",
            "522/522 [==============================] - 185s 354ms/step - loss: 0.7105 - accuracy: 0.7314 - val_loss: 1.3167 - val_accuracy: 0.5537\n",
            "Epoch 29/35\n",
            "522/522 [==============================] - 125s 240ms/step - loss: 0.6928 - accuracy: 0.7435 - val_loss: 1.3286 - val_accuracy: 0.5499\n",
            "Epoch 30/35\n",
            "522/522 [==============================] - 124s 238ms/step - loss: 0.6904 - accuracy: 0.7468 - val_loss: 1.3362 - val_accuracy: 0.5530\n",
            "Epoch 31/35\n",
            "522/522 [==============================] - 124s 238ms/step - loss: 0.6683 - accuracy: 0.7510 - val_loss: 1.3347 - val_accuracy: 0.5501\n",
            "Epoch 32/35\n",
            "522/522 [==============================] - 128s 245ms/step - loss: 0.6475 - accuracy: 0.7566 - val_loss: 1.4045 - val_accuracy: 0.5424\n",
            "Epoch 33/35\n",
            "522/522 [==============================] - 125s 240ms/step - loss: 0.6462 - accuracy: 0.7592 - val_loss: 1.3902 - val_accuracy: 0.5470\n",
            "Epoch 34/35\n",
            "522/522 [==============================] - 127s 244ms/step - loss: 0.6188 - accuracy: 0.7710 - val_loss: 1.3984 - val_accuracy: 0.5475\n",
            "Epoch 35/35\n",
            "522/522 [==============================] - 125s 239ms/step - loss: 0.6223 - accuracy: 0.7670 - val_loss: 1.4108 - val_accuracy: 0.5520\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x18e9bd71f40>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Saving Model For Future Use\n",
        "cp = ModelCheckpoint('model-ED', verbose=0, save_best_only=True)\n",
        "# Training Model\n",
        "model.fit(X_train, Y_train, epochs = 35,  callbacks=[cp], validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vaZ0JD8IVVL"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HVKgwFCIVVL"
      },
      "outputs": [],
      "source": [
        "face_detect=cv2.CascadeClassifier(r'C:\\AI\\AI\\haarcascades\\haarcascades\\haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQALuzhjIVVL"
      },
      "outputs": [],
      "source": [
        "source = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    not_to_use, image = source.read()\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detect.detectMultiScale(gray, 1.5,5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_roi = gray[y:y+w, x:x+w]\n",
        "        resized_face = cv2.resize(face_roi, (100, 100))\n",
        "        normalized_Face = resized_face/255\n",
        "        reshaped_face = np.reshape(normalized_Face, (1, 100, 100, 1))\n",
        "        result = model.predict(reshaped_face)[0]\n",
        "\n",
        "        # Using 5 Emotions as per definition\n",
        "\n",
        "        if np.amax(result)==result[0]:\n",
        "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "            cv2.putText(image,\"Angry\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(0,255,0),thickness=2)\n",
        "        if np.amax(result)==result[1]:\n",
        "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "            cv2.putText(image,\"happy\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(0,255,0),thickness=2)\n",
        "        if np.amax(result)==result[2]:\n",
        "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "            cv2.putText(image,\"neutral\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(0,255,0),thickness=2)\n",
        "        if np.amax(result)==result[3]:\n",
        "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "            cv2.putText(image,\"sad\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(0,255,0),thickness=2)\n",
        "        if np.amax(result)==result[4]:\n",
        "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "            cv2.putText(image,\"surprised\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(0,255,0),thickness=2)\n",
        "\n",
        "    Height=600\n",
        "    Width=600\n",
        "    dimension = (Width, Height)\n",
        "    resized_image = cv2.resize(image, dimension, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "    cv2.imshow('Verzeo Final Project', resized_image)\n",
        "    key = cv2.waitKey(1)\n",
        "    if key == 27:\n",
        "        source.release()\n",
        "        cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHDE8nMdIVVL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}